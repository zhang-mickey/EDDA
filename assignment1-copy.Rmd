---
title: "Assignment1"
author: "Yinghao Luo, Zheyuan Zhang, Yujie Cao"
output: pdf_document
date: "2025-02-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# only round 3
options(digits = 3)

knitr::opts_chunk$set(fig.height = 2.5)
```

## Exercise 1

**(a)** <!--Make some relevant plots of this data set, comment on normality. Investigate whether the columns Before and After8weeks are correlated.
--> For question 1a, we created several figures to check the characteristics of the dataset, which include histogram, Q-Q plot. According to the Q-Q plots, we can conclude that the data Before and After both follow a normal distribution, since the plots is approximately on a straight line.From the histogram,we can also make the same conclusion.

```{r, echo = FALSE}
data = read.table('cholesterol.txt', header = TRUE)
# Before
par(mfrow = c(1,2))
hist(data$Before, main="Before", xlab="Cholesterol (mmol/L)", col="lightblue", border="black")
qqnorm(data$Before, main="Q-Q Plot for Before 8 Weeks")
qqline(data$Before, col="red")

# After
hist(data$After8weeks, main="After 8 Weeks", xlab="Cholesterol (mmol/L)", col="lightgreen", border="black")
qqnorm(data$After8weeks, main="Q-Q Plot for After 8 Weeks")
qqline(data$After8weeks, col="red")

# 
```

Besides,we also check the scale,symmetry and outliers of the dataset through boxplot.

```{r}
par(mfrow = c(1,2)) 
boxplot(data$Before)
boxplot(data$After8weeks)
par(mfrow = c(1,1))
```

Then we also plot the scatter figures,we can see that there is approximately linear relation between these two values.

```{r}
plot(data$Before, data$After8weeks,
     main = "Scatter Plot of Before vs. After 8 Weeks",
     xlab = "Before", ylab = "After 8 Weeks",
     col = "black", pch = 16)
```

The correlation of these two columns(Before and After) can also be calculated by Pearson . Therefore, the value of correlation is 0.991, which can infer that they are correlated with each other.

```{r}

cor.test(data$Before, data$After8weeks, method="pearson")

```

```{=html}
<!--**(b)** Apply a couple of relevant tests (at least two tests, see Lectures 2–3) to verify whether the diet with low fat margarine has an eﬀect (argue whether the data are paired or not). Is a permutation test applicable? Is the Mann-Whitney test applicable?
-->
```

To answer the question about if the data are paired or not, the answer is yes. It is an experiment with two numerical outcomes per experimental unit. To be specific, the data are measured at two different time points, but within the same group of indiciduals. Therefore, **Mann-Whitney test is not applicable**, because it is utilized to compare two independent groups. However, the permutation test is applicable since we do not assume normality in a permutation test. As the sample size is 18,relatively small,we prefer non-Parametric tests.

First, we can conduct paired t-test as follows:

```{r}
t.test(data$Before,data$After8weeks, paired=TRUE)
```

The p-value is 3e-11, so we can know taht $H_0$ can be rejected. $H_0$ means no significant difference between Before and After8weeks. In this case, there is significant difference between Before and After8weeks.

Then, we can use permutation test as follows:

```{r}
mean_difference = function(x,y) {mean(x-y)}

original_diff <- mean(data$Before - data$After8weeks)

B=1000; tstar=numeric(B)

for (i in 1:B) {
  md_star=t(apply(cbind(data$Before,data$After8weeks),1,sample))
  tstar[i]=mean_difference(data$Before,data$After8weeks) 
}

myt = mean_difference(data$Before,data$After8weeks);myt

pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
p=2*min(pl,pr); p
```

In the permutation test, we can reject $H_0$ and the result also demonstrates that the diet with low fat margarine has a significant effect.

**c**

First, we construct the a 97%-CI for $\mu$ based on normality. As $X_1, \ldots, X_{18} \sim N(\mu,\sigma^2)$ , we can calculate the t-confidence interval of level $1-\alpha$ for $\mu$ . Note, $\alpha$ is 3% now.

```{r}
x = data$After8weeks
n = length(x)
x_mean = mean(x)
x_sd = sd(x) 
alpha = 0.03

t = qt(1-alpha/2, df = n-1)

CI_normal = c(
  x_mean - t * (x_sd / sqrt(n)),
  x_mean + t * (x_sd / sqrt(n))
)
cat("Normality 97% confidence interval: [", round(CI_normal[1], 3), 
    ",", round(CI_normal[2], 3), "]\n")
```

Then, we implement bootstrap CI as follows:

```{r}

B = 1000
x_mean = mean(data$After8weeks)
Tstar = numeric(B)
for(i in 1:B){
  Xstar=sample(data$After8weeks,replace=TRUE)
  Tstar[i]=mean(Xstar)
}
Tstar15 = quantile(Tstar, 0.015)
Tstar985 = quantile(Tstar, 0.985)

CI_bootstrap = c(2*x_mean-Tstar985,2*x_mean-Tstar15)
cat("Bootstrap 97% confidence interval: [", round(CI_bootstrap[1], 3), 
    ",", round(CI_bootstrap[2], 3), "]\n")

```

The difference between the two confidence intervals is slight. In my view, this might be because after8weeks conforms to a normal distribution.

**d**

We can use bootstrap method first as follows: The result may be slightly different for each bootstrap test. We choose the maximum of the sample as the test statistic and set B as 1000.

```{r}
n=length(data$After8weeks) # length
t=max(data$After8weeks) # the max of samples
B=1000;
tstar=numeric(B)
theta_values = seq(3, 12, by = 0.1)

p_values = sapply(theta_values, function(theta) {
  tstar = numeric(B)
  for (i in 1:B){
    xstar = runif(n, min = 3, max = theta)
    tstar[i] = max(xstar)
  }
  pl = sum(tstar < t) / B
  pr = sum(tstar > t) / B
  p = 2 * min(pl, pr)
  return(p)
})

# This is to find the range of theta for p > 0.05 
theta_valid = theta_values[p_values > 0.05]

cat("The range of theta for H0 cannot be rejected: [", min(theta_valid), max(theta_valid),  "]\n")


```

In terms of the question: Can the Kolmogorov-Smirnov test be also applied for this question? My answer is no. The value of $\theta$ is not a known fixed value , so the empirical distribution function is not completely known. In this case, it violates one of the assumptions of KS test.

**e**

We can test whether the proportion of samples with cholesterol levels of less than 6 are significantly less than 50% after 8 weeks. We can set the orginal hypothesis $H_0$ : the median is 6, while $H_1$ is median is smaller than 6. The binomial test can be applied as follows:

```{r}
n = length(data$After8weeks)
below_6 = sum(data$After8weeks < 6);

binom.test(below_6,n,p=0.5,alt="l")
```

Besides, we can also implement Wilcoxon signed rank test.

```{r}
 wilcox.test(data$After8weeks,mu=6)
```

We can conclude that $H_0$ cannot be rejected, since the p-value is higher than 0.05.

Let's forward to next question: design and perform a test to check whether the fraction of the cholesterol levels after 8 weeks of low fat diet less than 4.5 is at most 25%.

We can set the original hypothesis $H_0$ as the percentage of cholesterol levels below 4.5 is less than 25%, while $H_1$ means the percentage of cholesterol levels below 4.5 is more than or equal to 25%.

```{r}
below_4.5 = sum(data$After8weeks <= 4.5)
n = length(data$After8weeks)

# check if percentage of cholesterol levels below 4.5 is greater then 25%
binom.test(below_4.5,n,p=0.25,alt="greater")
```

Therefore, we cannot reject the original hypothesis $H_0$ since the p-value is larger than 0.05. We can conclude that the percentage of cholesterol levels below 4.5 is at most 25%.

## Exercise 2

<!-- column Crops contains the value of crops, column Size the size of farm, column County the county and column Related reflects the fact whether landlord and tenant are related-->

**(a)** Before we perform relevant ANOVA models,we need to check whether the data meets the necessary assumptions We need to check normality for numeric columns.

```{r}
df <- read.table("crops.txt", header = TRUE)
df$County <- as.factor(df$County)
df$Related <- as.factor(df$Related)

shapiro.test(df$Crops)
shapiro.test(df$Size)

```

Since Anova is not robust to outliers,we first need to check if there are outliers.

```{r}
boxplot(df$Crops)
```

We check variance equality using leveneTest before applying anova model.As we can see p\>0.05,thus we can apply anova models.

```{r}
library(car)
leveneTest(Crops ~ County, data = df)
leveneTest(Crops ~ Related, data = df)
leveneTest(Crops ~ County:Related, data = df)
```

**ANOVA model** There are two main factors:Country and Related,we also include the interaction factor:County+Related.

**Tests the independent effects**

```{r}
anova_model1 <- aov(Crops ~ County + Related, data = df) 
summary(anova_model1)
```

**Interaction model**

```{r}
anova_model2 <- aov(Crops ~ County*Related, data = df) 
summary(anova_model2)
```

Since all p-values are large (≫ 0.05), we do not reject the null hypotheses, meaning there is no strong evidence that County or Related significantly impact Crops. As there is no significance for the interaction model,we choose the pure model to estimate the data.

```{r}
new_farm <- data.frame(
  County = factor(3, levels = c(1, 2, 3)), 
  Related = factor("no", levels = c("no", "yes"))
)

predicted_crops <- predict(anova_model1, newdata = new_farm, interval = "confidence")
print(predicted_crops)
```

Here we do the post-hoc check using TukeyHSD

```{r}
TukeyHSD(anova_model2)
```

The Tukey test results confirm that the differences in means are not statistically significant.

````{=html}
<!--```{r}
#install.packages("car")
library(car)
leveneTest(anova_model2)
```-->
````

Examining residuals can help assess if the model assumptions hold. We can see that it shows non-random pattern,which suggests that the model is not well-fitted and may omit variables.

```{r}
plot(fitted_values1,residuals , main = "Residuals vs. Fitted", 
     xlab = "Fitted Values", ylab = "Residuals", pch = 19)
abline(h = 0, col = "red", lty = 2)

```

We can see that residuals are normally distributed.

```{r}
residuals1 <- resid(anova_model1)
fitted_values1 <- fitted(anova_model1) 
qqnorm(residuals1)
```

**(b)** ANOVA models only assumes that crops only depend on categorical factors,put size into perspective, we consider different ANCOVA models:

```{r}
boxplot(df$Size,horizontal = TRUE)
```

**Tests the independent effects**

```{r}
ancova_main <- aov(Crops ~ County + Related + Size, data = df)
summary(ancova_main)
```

**Size × County Interaction**

```{r}
ancova_county_size <- aov(Crops ~ County * Size + Related, data = df)
summary(ancova_county_size)
```

**Size × Related Interaction**

```{r}
ancova_related_size <- aov(Crops ~ County + Related * Size, data = df)
summary(ancova_related_size)
```

```{r}
AIC(ancova_main, ancova_county_size, ancova_related_size)
```

We can see from p-values that Size × County interaction terms is significant and this model have low AIC,thus we choose Size × County Interaction model.

**(c)** We can review the ANCOVA Model Results.

Significant County (p \< 0.05) → County influences on Crops,meaning crop yields differ across counties..

Significant Size (p \< 0.05) → Farm Size has a strong effect on Crops,which makes sense—larger farms generally produce more.

Significant Related (p \> 0.05) → Whether the landlord and tenant are related does not significantly impact the crops.

Significant County × Size (p \< 0.05) → This suggests that the effect of farm size on crop yield depends on the county. The figure blow confirms this conclusion,as the slopes are different.

```{r}
library(ggplot2)
ggplot(df, aes(x = Size, y = Crops, color = County)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Effect of Size on Crops Across Counties",
       x = "Farm Size", y = "Crops Yield")
```

**(d)**

```{r}
new_farm <- data.frame(
  County = factor(2, levels = c(1, 2, 3)), 
  Related = factor("yes", levels = c("no", "yes")),
  Size = 165  
)

predicted_crops <- predict(ancova_county_size, newdata = new_farm, interval = "confidence")
print(predicted_crops)

```

We can see that residuals are approximately normally distributed.

```{r}

residuals <- resid(ancova_county_size)
fitted_values <- fitted(ancova_county_size) 
qqnorm(residuals)
qqline(residuals, col = "red")
```

```{r}
plot(fitted_values, residuals, 
     main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", 
     ylab = "Residuals")
abline(h = 0, col = "red", lty = 2) # Add a horizontal line at 0
```

We estimate the error variance.

```{r}
error_variance <- sum(residuals(ancova_county_size)^2) / df.residual(ancova_county_size)
print(error_variance)
```
